{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 14:23:10.350533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 14:23:10.948202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys   \n",
    "\n",
    "from Bio import SeqIO\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-05\n",
    "NUMS_LABELS = 6\n",
    "OUTPUT_SIZE = 6\n",
    "LABEL_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(string_list):\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each string in the list\n",
    "    for string in string_list:\n",
    "        # Create a dictionary to hold the letters of the string\n",
    "        string_dict = {}\n",
    "\n",
    "        # Iterate over each letter in the string\n",
    "        for i, letter in enumerate(string):\n",
    "            # Create column name (e.g., 'Letter 1', 'Letter 2', etc.)\n",
    "            col_name = f'label{i+1}'\n",
    "\n",
    "            # Add the letter to the dictionary\n",
    "            string_dict[col_name] = letter\n",
    "\n",
    "        # Append the dictionary as a row to the DataFrame\n",
    "        df = df.append(string_dict, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_sequence(sequence, max_length):\n",
    "    if len(sequence) <= max_length:\n",
    "        return sequence\n",
    "    else:\n",
    "        return sequence[-max_length:]\n",
    "\n",
    "def process_dataset(dataset, max_length=1800):\n",
    "    processed_dataset = []\n",
    "    for sequence in dataset:\n",
    "        processed_sequence = truncate_sequence(sequence, max_length)\n",
    "        processed_dataset.append(processed_sequence)\n",
    "    return np.array(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets=[]\n",
    "val_outputs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df_x, df_y, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_x = df_x\n",
    "        self.df_y = df_y\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df_x = str(self.df_x[index])\n",
    "        df_x = \" \".join(df_x.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            df_x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'features': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'labels': torch.FloatTensor(self.df_y[index])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, LABEL_LENGTH)\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        if input_ids.shape[1] > 256:\n",
    "            num_sub_sequences = (input_ids.shape[1] - 1) // 256 + 1\n",
    "            outputs = []\n",
    "            for i in range(num_sub_sequences):\n",
    "                start_idx = i * 256\n",
    "                end_idx = min((i + 1) * 256, input_ids.shape[1])\n",
    "                sub_input_ids = input_ids[:, start_idx:end_idx]\n",
    "                sub_attn_mask = attn_mask[:, start_idx:end_idx]\n",
    "                sub_token_type_ids = token_type_ids[:, start_idx:end_idx]\n",
    "\n",
    "                output = self.bert_model(\n",
    "                    input_ids=sub_input_ids, \n",
    "                    attention_mask=sub_attn_mask, \n",
    "                    token_type_ids=sub_token_type_ids\n",
    "                )\n",
    "                output_dropout = self.dropout(output.pooler_output)\n",
    "                outputs.append(output_dropout)\n",
    "            print(outputs)\n",
    "            output_final = torch.cat(outputs, dim=1)\n",
    "            output = self.linear(output_final)\n",
    "        \n",
    "        else:\n",
    "            output = self.bert_model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attn_mask, \n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            output_dropout = self.dropout(output.pooler_output)\n",
    "            output = self.linear(output_dropout)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "Evaluation on the new test set:\n",
      "Test Loss: 0.5595 Test Accuracy: 0.4933 ROC Macro: 0.5430 ROC Micro: 0.5091 ap: 0.4601\n"
     ]
    }
   ],
   "source": [
    "# Load the model for evaluation on another test set\n",
    "model = BERTClass()\n",
    "model.load_state_dict(torch.load(\"bert_model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the new test set\n",
    "filename_test = \"mus_lncRNA_multi6_seq.fasta\"\n",
    "sequences_test = SeqIO.parse(filename_test, \"fasta\")\n",
    "X_test_new, y_test_new = [], []\n",
    "for record in sequences_test:\n",
    "    output = ' '.join(record.seq)\n",
    "    X_test_new.append(output)\n",
    "    y_test_new.append(record.id[:LABEL_LENGTH])\n",
    "\n",
    "df = create_dataframe(y_test_new)\n",
    "\n",
    "col_name = [f'label{i+1}' for i in range(LABEL_LENGTH)]\n",
    "for col_n in col_name:\n",
    "    df[col_n] = df[col_n].astype(str).astype(int)\n",
    "\n",
    "df.insert(loc=0, column='sequence', value=X_test_new)\n",
    "pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "\n",
    "LABEL_COLUMNS = ['label1', 'label2', 'label3']\n",
    "df[LABEL_COLUMNS].sum()\n",
    "\n",
    "X_test_new, y_test_new = df['sequence'], df[LABEL_COLUMNS]\n",
    "X_test_new, y_test_new = X_test_new.to_numpy(), y_test_new.to_numpy()\n",
    "\n",
    "X_test_new = process_dataset(X_test_new, max_length=1206)\n",
    "\n",
    "# Convert data to tensors and create DataLoader\n",
    "test_dataset_new = CustomDataset(X_test_new, y_test_new, tokenizer, MAX_LEN)\n",
    "test_loader_new = torch.utils.data.DataLoader(\n",
    "    test_dataset_new,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Evaluation on the new test set\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_targets = []\n",
    "test_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader_new):\n",
    "        features = data['features'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        labels = data['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "        outputs = model(features, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * loss.item())\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        predicted_labels = torch.round(torch.sigmoid(outputs))\n",
    "        test_correct += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "        test_targets.extend(labels.cpu().detach().numpy().tolist())\n",
    "        test_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "# Calculate average loss and accuracy on the new test set\n",
    "test_loss = test_loss / len(test_loader_new)\n",
    "test_accuracy = test_correct / (len(test_loader_new.dataset) * labels.shape[1])\n",
    "\n",
    "# Calculate ROC scores on the new test set\n",
    "test_targets = np.array(test_targets)\n",
    "test_outputs = np.array(test_outputs)\n",
    "test_roc_macro = roc_auc_score(test_targets, test_outputs, average='macro')\n",
    "test_roc_micro = roc_auc_score(test_targets, test_outputs, average='micro')\n",
    "test_ap = average_precision_score(test_targets, test_outputs)\n",
    "\n",
    "# Print evaluation metrics on the new test set\n",
    "print(\"Evaluation on the new test set:\")\n",
    "print(\"Test Loss: {:.4f} Test Accuracy: {:.4f} ROC Macro: {:.4f} ROC Micro: {:.4f} ap: {:.4f}\"\n",
    "      .format(test_loss, test_accuracy, test_roc_macro, test_roc_micro, test_ap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

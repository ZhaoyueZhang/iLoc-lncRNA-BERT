{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys   \n",
    "\n",
    "from Bio import SeqIO\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-05\n",
    "NUMS_LABELS = 6\n",
    "OUTPUT_SIZE = 6\n",
    "LABEL_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(string_list):\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each string in the list\n",
    "    for string in string_list:\n",
    "        # Create a dictionary to hold the letters of the string\n",
    "        string_dict = {}\n",
    "\n",
    "        # Iterate over each letter in the string\n",
    "        for i, letter in enumerate(string):\n",
    "            # Create column name (e.g., 'Letter 1', 'Letter 2', etc.)\n",
    "            col_name = f'label{i+1}'\n",
    "\n",
    "            # Add the letter to the dictionary\n",
    "            string_dict[col_name] = letter\n",
    "\n",
    "        # Append the dictionary as a row to the DataFrame\n",
    "        df = df.append(string_dict, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_sequence(sequence, max_length):\n",
    "    if len(sequence) <= max_length:\n",
    "        return sequence\n",
    "    else:\n",
    "        return sequence[-max_length:]\n",
    "\n",
    "def process_dataset(dataset, max_length=1800):\n",
    "    processed_dataset = []\n",
    "    for sequence in dataset:\n",
    "        processed_sequence = truncate_sequence(sequence, max_length)\n",
    "        processed_dataset.append(processed_sequence)\n",
    "    return np.array(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets=[]\n",
    "val_outputs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df_x, df_y, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df_x = df_x\n",
    "        self.df_y = df_y\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df_x = str(self.df_x[index])\n",
    "        df_x = \" \".join(df_x.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            df_x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'features': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'labels': torch.FloatTensor(self.df_y[index])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, LABEL_LENGTH)\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        if input_ids.shape[1] > 256:\n",
    "            num_sub_sequences = (input_ids.shape[1] - 1) // 256 + 1\n",
    "            outputs = []\n",
    "            for i in range(num_sub_sequences):\n",
    "                start_idx = i * 256\n",
    "                end_idx = min((i + 1) * 256, input_ids.shape[1])\n",
    "                sub_input_ids = input_ids[:, start_idx:end_idx]\n",
    "                sub_attn_mask = attn_mask[:, start_idx:end_idx]\n",
    "                sub_token_type_ids = token_type_ids[:, start_idx:end_idx]\n",
    "\n",
    "                output = self.bert_model(\n",
    "                    input_ids=sub_input_ids, \n",
    "                    attention_mask=sub_attn_mask, \n",
    "                    token_type_ids=sub_token_type_ids\n",
    "                )\n",
    "                output_dropout = self.dropout(output.pooler_output)\n",
    "                outputs.append(output_dropout)\n",
    "            print(outputs)\n",
    "            output_final = torch.cat(outputs, dim=1)\n",
    "            output = self.linear(output_final)\n",
    "        \n",
    "        else:\n",
    "            output = self.bert_model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attn_mask, \n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            output_dropout = self.dropout(output.pooler_output)\n",
    "            output = self.linear(output_dropout)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], device='cuda:2')\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], device='cuda:2')\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], device='cuda:2')\n",
      "Fold 1 Epoch:1/100 Loss:0.3688 Accuracy:0.6528 ROC Macro: 0.4402 ROC Micro: 0.7396 ap: 0.4741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2802909/3052673633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# bake-in time before making it the default, even if it is typically faster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_to_fused_or_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_foreach_supported_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0;31m     foreach = not fused and all(\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_foreach_supported_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_foreach_supported_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0;31m     foreach = not fused and all(\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_foreach_supported_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = \"homo_lncRNA_multi6_seq.fasta\"\n",
    "sequences = SeqIO.parse(filename, \"fasta\")\n",
    "\n",
    "X, y = [], []\n",
    "for record in sequences:\n",
    "    output = ' '.join(record.seq)\n",
    "    X.append(output)\n",
    "    y.append(record.id[:LABEL_LENGTH])\n",
    "\n",
    "df = create_dataframe(y)\n",
    "\n",
    "col_name = [f'label{i+1}' for i in range(LABEL_LENGTH)]\n",
    "for col_n in col_name:\n",
    "    df[col_n] = df[col_n].astype(str).astype(int)\n",
    "\n",
    "df.insert(loc=0, column='sequence', value=X)\n",
    "pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "\n",
    "LABEL_COLUMNS = ['label1', 'label2', 'label3']\n",
    "df[LABEL_COLUMNS].sum()\n",
    "\n",
    "X, y = df['sequence'], df[LABEL_COLUMNS]\n",
    "X, y = X.to_numpy(), y.to_numpy()\n",
    "X = process_dataset(X, max_length=1206)\n",
    "\n",
    "foldperf={}\n",
    "fold = 0\n",
    "\n",
    "k_fold = IterativeStratification(n_splits=10, order=1)\n",
    "for train, test in k_fold.split(X, y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    train_dataset = CustomDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "    test_dataset = CustomDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "        )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "        )\n",
    "\n",
    "    model = BERTClass()\n",
    "    model.to(device)\n",
    "   \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    history = {'test_loss': [], 'test_acc':[], 'test_macro': [], 'test_micro': [], 'test_ap': []}\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        train_correct = 0\n",
    "        test_correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            ids = data['features'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            labels = data['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += ((1 / (batch_idx + 1)) * loss.item())\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            predicted_labels = torch.round(torch.sigmoid(outputs))\n",
    "            train_correct += torch.sum(predicted_labels == labels).item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_targets = []\n",
    "            test_outputs = []\n",
    "            for batch_idx, data in enumerate(test_loader):\n",
    "                features = data['features'].to(device, dtype=torch.long)\n",
    "                mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "                labels = data['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "                outputs = model(features, mask, token_type_ids)\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                test_loss = test_loss + ((1 / (batch_idx + 1)) * loss.item())\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                predicted_labels = torch.round(torch.sigmoid(outputs))\n",
    "                test_correct += torch.sum(predicted_labels == labels).item()\n",
    "\n",
    "                test_targets.extend(labels.cpu().detach().numpy().tolist())\n",
    "                test_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "                \n",
    "            # Calculate average losses and accuracies\n",
    "            test_loss = test_loss / len(test_loader)\n",
    "            test_accuracy = test_correct / (len(test_loader.dataset) * labels.shape[1])\n",
    "\n",
    "            # Calculate ROC scores\n",
    "            test_targets = np.array(test_targets)\n",
    "            test_outputs = np.array(test_outputs)\n",
    "            test_roc_macro = roc_auc_score(test_targets, test_outputs, average='macro')\n",
    "            test_roc_micro = roc_auc_score(test_targets, test_outputs, average='micro')\n",
    "            test_ap = average_precision_score(test_targets, test_outputs)\n",
    "\n",
    "            # Print training/validation statistics\n",
    "            print(\"Fold {} Epoch:{}/{} Loss:{:.4f} Accuracy:{:.4f} ROC Macro: {:.4f} ROC Micro: {:.4f} ap: {:.4f}\"\n",
    "                    .format(fold+1, epoch, EPOCHS, test_loss, test_accuracy, test_roc_macro, test_roc_micro, test_ap))\n",
    "\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_accuracy)\n",
    "            history['test_macro'].append(test_roc_macro)\n",
    "            history['test_micro'].append(test_roc_micro)\n",
    "            history['test_ap'].append(test_ap)\n",
    "            \n",
    "    foldperf['fold{}'.format(fold+1)] = history \n",
    "    fold += 1\n",
    "  \n",
    "roc_micro_mean = np.mean([history['test_micro'][-1] for history in foldperf.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 10 fold cross validation\n",
      "Average Loss: 0.1770 \t Accuracy: 0.6941 \t Macro: 0.5435 \t Micro: 0.7545 \t ap: 0.5469\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, test_macro, test_micro, test_ap = [], [], [], [], []\n",
    "k=10\n",
    "for f in range(1,k+1):\n",
    "\n",
    "     test_loss.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "     test_accuracy.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "     test_macro.append(np.mean(foldperf['fold{}'.format(f)]['test_macro']))\n",
    "     test_micro.append(np.mean(foldperf['fold{}'.format(f)]['test_micro']))\n",
    "     test_ap.append(np.mean(foldperf['fold{}'.format(f)]['test_ap']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Loss: {:.4f} \\t Accuracy: {:.4f} \\t Macro: {:.4f} \\t Micro: {:.4f} \\t ap: {:.4f}\".format(np.mean(test_loss),np.mean(test_accuracy),np.mean(test_macro),np.mean(test_micro),np.mean(test_ap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"bert_model.pt\"\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
